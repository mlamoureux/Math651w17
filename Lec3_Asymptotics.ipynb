{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Lecture 3 - Asymptotics\n",
    "\n",
    "## Math 651 - University of Calgary\n",
    "## Mathematical Modeling for Industry\n",
    "## Winter 2017\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Asympotic expansions\n",
    "\n",
    "The idea of an asymptotic expansion is to perturb a given system by a small parameter $\\epsilon$ (or perhaps by a small function $\\epsilon = \\epsilon(x,y,z,etc)$), and say something concrete about the resulting solution. Even if you can't compute the exact solution. \n",
    "\n",
    "This is somewhat similar to what we see in Taylor polynomial expansions for analytic functions. For instance, we know $\\sin(0) = 0$ and a first order approximation for small argument values is\n",
    "$$\\sin(\\epsilon) \\approx \\epsilon$$\n",
    "while a third order approximation is\n",
    "$$\\sin(\\epsilon) \\approx \\epsilon - \\frac{\\epsilon^3}{6}.$$\n",
    "What is different in general asymptotics is that we can't necessarily extend the approximation to an exact infinite series formula, and we might use something other than polynomials in the various orders of expansions. Also, we typically use a bootstrapping method where we assume the solution has some sort of asymptotic expansion, then find that expansion without every actually computing the solution!\n",
    "\n",
    "An example might help to clarity these ideas.\n",
    "\n",
    "### Simple example.\n",
    "Can we solve for $x$ in the equation\n",
    "$$\\epsilon x^2 + x - 1 = 0$$\n",
    "when the parameter $\\epsilon$ is small? Well, of course we can because we know the quadratic formula (don't we?). \n",
    "\n",
    "But, if we didn't have the quadratic formula, we would notice that for $\\epsilon = 0$ the equation simplifies to \n",
    "$$ x-1 = 0$$\n",
    "which has the unique solution $x=1$. Plugging that back into the original equation, we see we have\n",
    "$$ \\epsilon (1)^2  + 1 - 1 = 0$$\n",
    "which tells use $\\epsilon = 0$, as expected. So we might guess that the solution for non-zero $\\epsilon$ is given by some expansion\n",
    "$$ x = x_0 + \\epsilon x_1 + \\epsilon^2 x_2 + \\epsilon^3 x_3 + \\cdots.$$\n",
    "Substituting into the original equation gives\n",
    "$$\\epsilon (x_0 + \\epsilon x_1 + \\epsilon^2 x_2  + \\cdots)^2 + \n",
    "x_0 + \\epsilon x_1 + \\epsilon^2 x_2  + \\cdots - 1 = 0.$$\n",
    "Collect powers of $\\epsilon$ to get\n",
    "$$ (x_0-1)\\epsilon^0 = (x_0^2 + x_1)\\epsilon^1 + (x^2 + 2x_0x_1)\\epsilon^2 + \\cdots = 0.$$\n",
    "Equate successive powers of $\\epsilon$ to find\n",
    "- $\\epsilon^0$ gives $x_0-1=0$ which implies $x_0 = 1$.\n",
    "- $\\epsilon^1$ gives $x_1 + x_0^2=0$ which implies $x_1 = -1$.\n",
    "- $\\epsilon^2$ gives $x_2 + 2x_0x_1=0$ which implies $x_2 = 2$.\n",
    "\n",
    "And so on. So we have an expansion\n",
    "$$ x = 1 - \\epsilon + 2\\epsilon^2 + \\cdots.$$\n",
    "This is not surprising, as it is also obtained by using a Taylor series expansion for a square root, in the quadratic formula\n",
    "$$x = \\frac{-1 + \\sqrt{1 + 4\\epsilon}}{2\\epsilon} \\approx \n",
    "\\frac{-1 + 1 + \\frac{1}{2}(4\\epsilon) - \\frac{1}{8}(4\\epsilon)^2 + \\frac{1}{16}(4\\epsilon)^3}{2\\epsilon} = 1 - \\epsilon + 2\\epsilon^2.$$\n",
    "\n",
    "But, something curious happened here. We only have one root, and quadratics should have two roots. What went wrong?\n",
    "\n",
    "Well, once we chose $x$ to balance the $1$ in the equation, we made a tacit assumption that these two terms (from the eqn $\\epsilon x^2 + x - 1 = 0$) more or less cancel in any asympotic expansion. And so the third term $\\epsilon x^2$ in the original quadratic has to be small. What if we choose a different balancing of the three terms $\\epsilon x^2, x, 1$? Suppose we assume that $\\epsilon x^2$ and $x$ balance in the expansion. Then their ratio gives\n",
    "$$ \\frac{\\epsilon x^2}{x} = \\epsilon x \\approx 1,$$\n",
    "and so we conclude\n",
    "$$ x \\approx \\frac{1}{\\epsilon}$$\n",
    "is in this case a very large term. Thus we should rescale the problem as\n",
    "$$ x = \\frac{X}{\\epsilon}.$$\n",
    "\n",
    "Now, the original equation\n",
    "$$ \\epsilon x^2 + x - 1 = 0$$\n",
    "rescales to \n",
    "$$\\epsilon \\frac{X^2}{\\epsilon^2} + \\frac{X}{\\epsilon} - 1 = 0$$\n",
    "or equivalently \n",
    "$$X^2 + X - \\epsilon = 0.$$\n",
    "Doing an expansion $X = X_0 + \\epsilon X_1 + \\epsilon^2 X_2 + \\cdots$, we plug in as\n",
    "$$(X_0 + \\epsilon X_1 + \\epsilon^2 X_2 + \\cdots)^2 + \n",
    "(X_0 + \\epsilon X_1 + \\epsilon^2 X_2 + \\cdots) - \\epsilon = 0.$$\n",
    "Collect terms and equate successive powers of $\\epsilon$ to find\n",
    "- $\\epsilon^0$ gives $X_0^2+X_0=0$ which has two solutions, $X_0 = -1$ or $X_0 = 0$.\n",
    "- $\\epsilon^1$ gives $2X_0X_1 + X_1 - 1=0$ which has corresponding solns $X_1 = -1$ or $1$.\n",
    "- $\\epsilon^2$ gives $2X_0X_2 + X_1^2 + X_2=0$ which has corresponding solns $X_2 = 1$ or $-1$.\n",
    "\n",
    "So we have two expansions,\n",
    "$$ X = -1  - \\epsilon + \\epsilon^2 + \\cdots \\mbox{ scales to } x=\\frac{-1}{\\epsilon} -1 + \\epsilon \\cdots,$$\n",
    "which is the new, missing solution (and is big!), and\n",
    "$$ X = 0 + \\epsilon - \\epsilon^2 + \\cdots \\mbox{ scales to } x = 1 - \\epsilon + \\cdots,$$\n",
    "which is the same as the first solution we obtained before. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: if you didn't like that substitution $x = \\frac{X}{\\epsilon}$ you could also get the second solution by plugging in a expansion:\n",
    "$$ x = \\frac{x_{-1}}{\\epsilon} + x_0 + \\epsilon x_1 + \\epsilon^2 x_2 + \\epsilon^3 x_3 + \\cdots$$\n",
    "and then solve for the various coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some key points in this asymptotic expansion\n",
    "1. We are embedding our one problem into a continuous family of problems, parameterized by all $\\epsilon$ in some interval $[0, \\epsilon^*]$. If the solutions have some smooth structure as $\\epsilon\\rightarrow 0$, we should be able to extract their general behaviour.\n",
    "\n",
    "2. Systematic approximation begins with an identification of the dominant balance in the equation. Physical and mathematical intuition can guide us here, and the answer may not be unique. \n",
    "\n",
    "3. It may be necessary to rescale some dependent or independent variables, in order to achieve a proper balance in the equation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Order Notation (Big O, little o)\n",
    "We this idea that some approximations are better than others, and we measure that \"goodness\" by the order of the approximation. To make this precise, we have to define what order is. In fact, we have three closely related definitions: big O, little o, and $\\sim$. Usually, we are interested in the case where some well-known function $g(x)$ goes to zero at a point $x_0$, and we want to say some other function $f(x)$ also goes to zero, at a similar rate. We make this precise with the following definitions. \n",
    "\n",
    "### Definition of Big O: $f(x) = O(g(x)) \\mbox{ as } x\\rightarrow x_0$\n",
    "means there is a constant $A>0$ so that $|f(x)| \\leq A\\cdot |g(x)|$ for all $x$ near $x_0$. \n",
    "\n",
    "### Definition of little o: $f(x) = o(g(x)) \\mbox{ as } x\\rightarrow x_0$\n",
    "means $\\lim_{x\\rightarrow x_0} \\frac{f(x)}{g(x)} = 0.$\n",
    "\n",
    "### Definition of $\\sim$: $f(x) \\sim g(x) \\mbox{ as } x\\rightarrow x_0$\n",
    "means $\\lim_{x\\rightarrow x_0} \\frac{f(x)}{g(x)} = 1.$\n",
    "\n",
    "So, for instance, we can say that\n",
    "$$\\sin(x) \\sim x \\mbox{ as $x\\rightarrow 0$}.$$\n",
    "We can also say that as an approximation, the difference between $\\sin(x)$ and $x$ is $O(x^3)$, because it is true that $|\\sin(x) - x| < 0.5 x^3$ for $x$ small. We write\n",
    "$$\\sin(x) - x = O(x^3) \\mbox{ as $x\\rightarrow 0$}.$$\n",
    "We can also say that\n",
    "$$\\sin(x) - x = o(x^2) \\mbox{ as $x\\rightarrow 0$},$$\n",
    "since the limit of $(\\sin(x)-x)/x^2$ is zero, as $x\\rightarrow 0$. \n",
    "\n",
    "In our quadratic example above, we have two solutions $x^{(1)} = 1 - \\epsilon + \\cdots$, and\n",
    "$x^{(2)} = 1/\\epsilon - 1 + \\epsilon + \\cdots$. So we can say that $x^{(1)} = O(1)$ while $x^{(2)} = O(1/\\epsilon),$ as $\\epsilon$ tends to zero. Notice in this case, the $x^{(2)}$ is tending to infinity at a specified rate. \n",
    "\n",
    "\n",
    "## Definition of asymptotic sequence\n",
    "In the examples above, we used monomials $x^n$ or $\\epsilon^n$ as a measure of the order of how fast something is going to zero (or infinity). This is an orderly way to do things, because we see that\n",
    "$$\\epsilon^{n+1} = o(\\epsilon^n) \\mbox{ as } \\epsilon\\rightarrow 0.$$\n",
    "But we don't have to restrict to monomials - we could use fractional powers of $\\epsilon$, or different exponentials, etc, whatever is convenient for the problem at hand. We just need to generalize our notion of what is an appropriate sequence of functions to measure order.\n",
    "\n",
    "### Definition\n",
    "An asymptotic sequence (as $\\epsilon\\rightarrow 0$) is an indexed collection of functions $\\{ \\phi_n(\\epsilon) \\}$, $n = 0,1,2,\\ldots$ which satisfies\n",
    "$$ \\phi_{n+1}(\\epsilon) = o (\\phi_n(\\epsilon)) \\mbox{ as $\\epsilon\\rightarrow 0$, for all $n$.}$$\n",
    "\n",
    "For instance, $\\{ \\epsilon^n \\}, n=0,1,2,\\ldots $ is an asymptotic sequence, as is $\\{ \\epsilon^{n/2} \\}, n=0,1,2,\\ldots$. But so is the sequence $\\{ \\exp(-n/\\epsilon) \\}, n=0,1,2,\\ldots $, if we keep the $\\epsilon$ positive. This gives a different kind of order of decay (more exponential). \n",
    "\n",
    "## Definition of asymptotic expansion\n",
    "\n",
    "We say a function $f(\\epsilon)$ has an asymptotic expansion with respect to the asymptotic sequence $\\{ \\phi_n(\\epsilon) \\}$, $n = 0,1,2,\\ldots$ if there exists constants $a_k$ such that\n",
    "$$f(\\epsilon) = \\sum_{k=0}^n a_k \\phi_k(\\epsilon) + o(\\phi_n(\\epsilon)), \\mbox{ as $\\epsilon\\rightarrow 0$, for all $n$}.$$\n",
    "Equivalently, \n",
    "$$f(\\epsilon) \\sim \\sum_{k=0}^n a_k \\phi_k(\\epsilon), \\mbox{ for all $n$}.$$ \n",
    "\n",
    "So, for instance, Taylor series are a familiar example of an asymptotic expansion, as we can write\n",
    "$$\\exp(x) \\sim 1 + x + \\frac{1}{2!} x^2 + \\cdots + \\frac{1}{n!} x^n.$$\n",
    "However, be careful! An asymptotic expansion does not necessarily coverge to $f$ as we let $n$ tend to infinity, unlike nice Taylor series. Despite this, they are very useful approximations. \n",
    "\n",
    "\n",
    "## The example $\\int_0^\\epsilon \\frac{e^{-1/t}}{t} \\, dt.$\n",
    "\n",
    "This is a neat problem. You can't solve the integral exactly, and yet the integrand is pretty well behaved near zero, since the $1/t$ that blows up is rapidly cancelled by the exponential factor on top. So for small epsilon, we should have a nice expansion in terms of powers of epsilon. Taylor series won't work, because of the $1/t$ in the argument of the exponention. \n",
    "\n",
    " This example is derived from a similar problem in our text, but there they integrate out to infinity. I think this example with the small interval of integration is a better illustration of the use of asymptotics.\n",
    "\n",
    "So, looking at $\\int_0^\\epsilon \\frac{e^{-1/t}}{t} \\, dt,$ let's use integration by parts with $u = t$ and $dv = (1/t^2)e^{-1/t} dt.$ So $du = dt, v = e^{-1/t}$ and we get \n",
    "$$\\int_0^\\epsilon \\frac{e^{-1/t}}{t} \\, dt = te^{-1/t}\\biggr\\rvert_{t=0}^\\epsilon - \\int_0^\\epsilon e^{-1/t} \\, dt = \\epsilon e^{-1/\\epsilon} - \\int_0^\\epsilon e^{-1/t} \\, dt.$$\n",
    "Now the integral on the right is bounded by the length of the interval $[0,\\epsilon]$ times the maximum value of the exponential, which is $e^{-1/\\epsilon}$, so the result is of order $\\epsilon e^{-1/\\epsilon}$, exactly what we want. \n",
    "\n",
    "So we are on the right track. Do integration by parts on $\\int_0^\\epsilon e^{-1/t} \\, dt$, with $u = t^2$, $dv = (1/t^2)e^{-1/t}$ to get\n",
    "$$\\int_0^\\epsilon \\frac{e^{-1/t}}{t} \\, dt =\n",
    "\\epsilon e^{-1/\\epsilon} - t^2e^{-1/t}\\biggr\\rvert_{t=0}^\\epsilon + \\int_0^\\epsilon 2t e^{-1/t} \\, dt =\n",
    "\\epsilon e^{-1/\\epsilon} - \\epsilon^2 e^{-1/\\epsilon} + \\int_0^\\epsilon 2t e^{-1/t} \\, dt.$$\n",
    "Now we see the pattern. Each time we integrate by parts, we get a higher power of $t$ in the integrand, and pull out another integer factor. So, we obtain the asymptotic expansion\n",
    "$$\\int_0^\\epsilon \\frac{e^{-1/t}}{t} \\, dt =\n",
    "\\epsilon e^{-1/\\epsilon} - \n",
    " \\epsilon^2 e^{-1/\\epsilon} + \n",
    " 2!\\epsilon^3 e^{-1/\\epsilon} - \n",
    " 3!\\epsilon^4 e^{-1/\\epsilon} + \\cdots + (-1)^n n! \\epsilon^{n+1}e^{-1/\\epsilon} + o(e^{-1/\\epsilon}\\epsilon^{n+1}),$$\n",
    "or more simply, \n",
    "$$\\int_0^\\epsilon \\frac{e^{-1/t}}{t} \\, dt =\n",
    " \\left(\n",
    "1 - \\epsilon + 2!\\epsilon^2 - 3!\\epsilon^3 + \\cdots + (-1)^n n! \\epsilon^n + o(\\epsilon^n)\n",
    "\\right)\\epsilon e^{-1/\\epsilon}.$$\n",
    "\n",
    "Notice the infinite series $\\sum_0^\\infty (-1)^n n! \\epsilon^n$ diverges for all non-zero $\\epsilon$, so there is no use in trying to take the finite sum our asympotic expansion and  extend to an infinite sum. Nevertheless, the asymptotic series is still very useful. \n",
    "\n",
    "For instance, with $\\epsilon = 1/4$, a precise numerical calculation gives a value of $.00378\\ldots $ for the integral, while the $n=4$ asymptotic expansion gives the approximate value $.00401\\ldots.$ With $\\epsilon = 1/8$, the numerical calculation gives  $.000037717\\ldots $ for the integral, while the $n=8$ asymptotic expansion gives the approximate value $.000037666\\ldots,$ an error of only $0.14$%.\n",
    "\n",
    "The book mentions that for $\\epsilon$ of size about $1/n$, it is best to take the $n$-th order asymptotic expansion. I wonder why? You could look into it. (Recall from above we have an exact integral that represents the error in the n-th stage expansion, so maybe you can optimize that...)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The example of octave stretch in a piano\n",
    "This example is based on the fact that a piano wire is not a perfectly elastic spring, but has characteristics like a metal beam. There is a small parameter $\\epsilon$ in the problem, that measures the departure from perfect elasticity. This change actually affects the tuning of a real piano.\n",
    "\n",
    "\n",
    "Music notes are typically generated by vibrations in some object (the strings of a violin, the wires in a piano, the metal bars in a xylophone, even vibrations of an air column in a clarinet, flute, saxophone, etc.) The pitch of a note is characterized by a frequency of vibration -- for instance, concert \"A\" is set at 440 Hertz (cycles per second). Pairs of notes sound nice together if their frequencies are in the ratio of a small integer fraction. For instance, A is 440 Hz, and E is 660Hz, which is 3/2 the frequency of A. (This interval, from A to E, is called a perfect fifth. It is the first tone step you hear in the song \"Twinkle twinkle little star\", or in French \"Ah! vous dirai-je, maman.\") A leap by an octave is a doubling of frequency, such as 440Hz to 880 Hz (this the leap you hear in the tune \"SomeWhere over the rainbow\" from the Wizard of Oz.)\n",
    "\n",
    "A simple string vibrates with a fundamental frequency, as well with harmonics that are frequencies at integer multiples of the fundamental By tuning to the harmonics, it is possible to tune a piano by ear -- adjusting the tension on a string until the frequencies line up. However, a real piano has more complicated harmonics, so simple tuning gives a slight error in the alignment of the notes. Indeed, over the 8 octaves of a piano, the errors accumulate into something that is clearly noticable by ear.\n",
    "\n",
    "We will show here the PDE describing the vibrations of a simple string, and then consider the case of a real piano wire that has more complex vibrations. \n",
    "\n",
    "### Wave equation for a simple string\n",
    "The PDE governing the vibrations of a simple is derived from Newton's law (mass times acceleration equals force), giving\n",
    "$$\\rho A \\frac{\\partial^2 y}{\\partial t^2} = T \\frac{\\partial^2 y}{\\partial x^2}, \\mbox{ for $0<x<L$},$$\n",
    "where $y=y(x,t)$ is the (vertical) displacement of a string from rest, $\\rho$ is the density of the string, $A$ is the cross-sectional area of the string, and $T$ is the tension on the string (so the term $T \\frac{\\partial^2 y}{\\partial x^2}$ represents the force on the string due to the curvature of the vibrating string). It is pretty standard to rewrite the PDE as\n",
    "$$ \\frac{\\partial^2 y}{\\partial t^2} - c^2 \\frac{\\partial^2 y}{\\partial x^2}=0, \\mbox{ for $0<x<L$},$$ where\n",
    "$c^2 = T/\\rho A$ is the speed of propagation (squared) for vibrations in the string. \n",
    "\n",
    "\n",
    "We include the boundary conditions\n",
    "$$y(0) = 0, \\qquad y(L) = 0,$$\n",
    "where $L$ is the length of the string. \n",
    "\n",
    "By separating variables, we find the normal modes of vibration as\n",
    "$$y_n = e^{i\\omega_n t} \\sin\\frac{n\\pi }{L}x, \\mbox{ with } \\omega_n = \\frac{n\\pi }{L}c.$$\n",
    "So we see here the temporal frequencies $\\omega_n$ are simple integer multiples of the fundamental frequency $\\omega_1$.\n",
    "\n",
    "To get a non-dimensional version, we scale $x$ with $L$, $t$ with $L/c$, and dropping primes to get\n",
    "$$ \\frac{\\partial^2 y}{\\partial t^2} - \\frac{\\partial^2 y}{\\partial x^2}=0, \\mbox{ for $0<x<1$, with $y(0)=y(1)=0$}.$$\n",
    "Now the normal modes are \n",
    "$$y_n = e^{i\\Omega_n t} \\sin n\\pi x, \\mbox{ with } \\Omega_n = n\\pi.$$\n",
    "\n",
    "### Wave equation for a real piano wire\n",
    "A real piano wire is a bit more like a flexible beam, so there is a bending stiffness. From engineering beam models, we expect a PDE of the form\n",
    "$$ \\rho A \\frac{\\partial^2 y}{\\partial t^2} - T \\frac{\\partial^2 y}{\\partial x^2}\n",
    "+ E A k^2 \\frac{\\partial^4 y}{\\partial x^4} = 0,$$\n",
    "where $E$ is Young's modulus for the wire, and $k$ is the radius of gyration. \n",
    "\n",
    "We note that for a circular wire of radius $a$, that the radius of gyration is given by $k^2 = \\frac{1}{2}a^2.$\n",
    "\n",
    "Scaling $x,t$ as before, we obtain the equation\n",
    "$$  \\frac{\\partial^2 y}{\\partial t^2} -  \\frac{\\partial^2 y}{\\partial x^2}\n",
    "+ \\epsilon \\frac{\\partial^4 y}{\\partial x^4} = 0,$$\n",
    "where the dimensionless parameter $\\epsilon$ is given by\n",
    "$$\\epsilon - \\frac{Ek^2}{\\rho L^2 c^2} = \\frac{EAk^2}{TL^2}.$$\n",
    "\n",
    "Note that area $A$ is proportional to radius squared ($a^2$) as is $k^2$, so the parameter $\\epsilon$ depends on the fourth order of the radius of the wire. (i.e. skinny wires are closer to the simple string example above.) \n",
    "\n",
    "For a typical piano wire, we might have a radius of $a=1mm$, so $k^2 = \\frac{1}{2}\\times 10^{-6} m^2$, $L=1m$, $T=1000N$ (Newtons), and for steel we have a density $\\rho = 7800 kg/m^3$ and modulus $E = 2\\times 10^{11}$ in SI units. This gives\n",
    "$$\\epsilon \\approx 3.1\\times 10^{-4},$$\n",
    "so indeed it is a small parameter. \n",
    "\n",
    "The corresponding fundamental frequency, in the simple string, would be\n",
    "$$\\omega_1 = \\frac{\\pi c}{L} \\approx 280Hz,$$\n",
    "which is about middle C on the piano. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve a 4th order PDE, we need two more boundary conditions. It is reasonable to include the conditions\n",
    "$$\\frac{\\partial^2y}{\\partial x^2} = 0, \\mbox{ at } x=0, x=1,$$\n",
    "which represents a \"straight wire\" condition at the ends of the vibrating string. (Basically, the string oscillates about the bridge support, without curving.)\n",
    "\n",
    "Or, to be honest, maybe this is just some hack to ensure we still can use the sine function in the separation of variables. Think about this.\n",
    "\n",
    "Anyhow, we now try a solution using separation of variables, taking this function\n",
    "$$y_n = e^{i\\Omega_n t}\\sin n\\pi x$$\n",
    "and plugging into our fourth order PDE, to obtain the fourth order algebraic equation\n",
    "$$-\\Omega_n^2 + n^2\\pi^2 + \\epsilon n^4 \\pi^4 = 0.$$\n",
    "Thus the normal mode frequencies are given as \n",
    "$$\\Omega_n = n\\pi(1+ \\epsilon n^2 \\pi^2)^{1/2} \\approx n\\pi(1 + \\frac{1}{2}\\epsilon n^2 \\pi^2 + o(\\epsilon)),$$\n",
    "where we have used a first order approximation to the square root function. This is our first asymptotic expansion, where here we show how the harmonics depend on the small parameter $\\epsilon$. The harmonic frequencies do grow with $n$, but not quite linearly as there is an order epsilon correction.\n",
    "\n",
    "So already, we see the normal mode frequencies are not simple integer multiples of some basic frequency. \n",
    "\n",
    "To investigate further, we observe the fundamental frequency is\n",
    "$$\\Omega_1 \\approx \\pi(1 + \\frac{1}{2}\\epsilon \\pi^2), $$\n",
    "and the n-th harmonic has frequency\n",
    "$$\\Omega_n \\approx n\\Omega_1 \\frac{1+\\frac{1}{2}\\epsilon n^2\\pi^2}{1+\\frac{1}{2}\\epsilon \\pi^2}\n",
    "\\approx n\\Omega_1 \\left(1 + \\frac{1}{2}\\epsilon \\pi^2 (n^2-1) \\right).$$\n",
    "This is a second asymptotics expansion.  (Maybe you should work out the details yourself for this expansion. Where is the $n^2 - 1$ come from?)\n",
    "\n",
    "So again, we see the frequencies are approximately an integer multiple $n\\Omega_1$ of the fundamental frequency $\\Omega_1$, but there is a small correction.\n",
    "\n",
    "### Octave stretch\n",
    "So, in tuning a piano, we often tune by octaves, so the second harmonic $\\Omega_2$ is important. In our example, with $\\epsilon = 3.1\\times 10^{-4}$, we find the ratio $\\Omega_2/\\Omega_1$ is\n",
    "$$ 2(1 + \\frac{3}{2}\\epsilon \\pi^2) = 2\\cdot (1.00458).$$\n",
    "Such a small discrepency is not really noticable, but over 7 octaves we have\n",
    "$$ (1 + \\frac{3}{2}\\epsilon \\pi^2)^7 \\approx 1 + \\frac{21}{3}\\epsilon\\pi^2 \\approx 1.033,$$\n",
    "which is more than half a semitone discrepancy. Which is easily noticable by ear. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.0",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
